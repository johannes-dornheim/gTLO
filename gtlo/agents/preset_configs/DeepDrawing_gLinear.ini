[general parameters]
experiment_label = dd_linear

env_id = 2d-deepdrawing-5ts-stressoffsetstate-v0
# (from 'gTLO', linear_dqn)
agent_type = linear_dqn
# path to experiment storage
experiment_storage = ../experiment_out

# how many experiment-runs?
run_count = 10
# how many steps per experiment?
steps_count = 12500
# evaluate every n steps
eval_frequency = 1250

[morl parameters]
# hypervolume reference point
hypervolume_ref = [-0.1, -0.1]

# env reward is scaled accordingly
reward_scale = 0.1
# from 'eql' (equal in range), 'exact' (exact, depends on env)
weights_type = eql
# amount of weight-configs used during training and evaluation
weights_count = 100
# range of weights (assumes two term MORL-env)
weights_min = 0
weights_max = 1

outer_loop = False

[gtlo parameters]

[dqn parameters]
dqn_batches = 8
buffer_size = 12500
gamma = 0.9
target_network_update_freq = 1000
extension_priorreplay = True
learning_starts = 1000

[Deep Drawing parameters]
# order and usage of reward terms (from rt_thickness, rt_feeding, rt_mises)
reward_terms = ["rt_feeding", "rt_thickness"]

[DST parameters]
