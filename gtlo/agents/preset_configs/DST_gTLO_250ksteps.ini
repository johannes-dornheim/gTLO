[general parameters]
experiment_label = dst_gtlo_250k

# (from '2d-deepdrawing-5ts-stressoffsetstate-v0', fruity-gym-v0)
env_id = fruity-gym-v0
# (from 'gTLO', linear_dqn)
agent_type = gTLO

# path to experiment storage
experiment_storage = ../experiment_out

# how many experiment-runs?
run_count = 3
# how many steps per experiment?
steps_count = 250000
# evaluate every n steps
eval_frequency = 1000

[morl parameters]
# hypervolume reference point
hypervolume_ref = [0, -25]

# env reward is scaled accordingly
reward_scale = 0.1
# from 'eql' (equal in range), 'exact' (exact, depends on env)
weights_type = eql
# amount of weight-configs used during training and evaluation
weights_count = 100
# range of weights (assumes two term MORL-env)
weights_min = 0.05
weights_max = 10

outer_loop = False

[gtlo parameters]

[dqn parameters]
dqn_batches = 8
buffer_size = 250000
gamma = 1.0
target_network_update_freq = 5000
extension_priorreplay = False
learning_starts = 1000

[Deep Drawing parameters]
# order and usage of reward terms (from rt_thickness, rt_feeding, rt_mises)
reward_terms = ["rt_feeding", "rt_thickness"]

[DST parameters]
game_id = van2014multi
max_episode_steps = 50
render = True